{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ltYdRQqvp9"
      },
      "source": [
        "To generate the mosaic to classify using the CNN treined, clik on the google earth engine link: https://code.earthengine.google.com/797aa36ede2caf55a72f0bdc42dc4f35 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1GKkAiU3vbQ"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "After adding the shortcut to the data in your Google Drive, the next step is to mount a Google Drive volume on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt_Mg55M3tfS",
        "outputId": "f34183bf-5b88-4aa2-bdd7-674041f0076c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFa0jBMnWEIR"
      },
      "source": [
        "# Check GPU\n",
        "\n",
        "We recommend that the entire model and classification training process be done using some of the GPUs available from Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmsT7NZ0YWHC"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T7SsB0sFHZo"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdcHJiRBEiv4",
        "outputId": "b6383cff-9620-4f2e-b41f-e2d5d00ee1cd"
      },
      "outputs": [],
      "source": [
        "#Step 1\n",
        "!apt-get update\n",
        "#Step 2\n",
        "!apt-get install libgdal-dev -y\n",
        "#Step 3\n",
        "!apt-get install python-gdal -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ2koetlt85z"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptqj8M7pt_AZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join, sep, exists\n",
        "import gc\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "from osgeo import gdal, osr\n",
        "from skimage.transform import resize, rotate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, \\\n",
        "    Conv2DTranspose, concatenate, BatchNormalization, Activation\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.utils import compute_class_weight\n",
        "from skimage import exposure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV6wHkfjIoas"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClOeVNyeUHOV"
      },
      "source": [
        "> Chose the state to classify. \\\n",
        "Options: \\\n",
        "'RS', 'SCPR', 'TO'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjSCkwa0SHI4"
      },
      "outputs": [],
      "source": [
        "UF = 'TO' # <-------TO, SCPR or RS\n",
        "\n",
        "CHIP_SIZE = 256\n",
        "LABELS = [0, 1]\n",
        "SPATIAL_SCALE = 30\n",
        "PROJECTION = 3857 # pseudo-mercator\n",
        "REPROJECT = False\n",
        "\n",
        "# Train model\n",
        "#TRAIN_BATCH_SIZE = 16 # para  16\n",
        "TRAIN_BATCH_SIZE = 5 # para 64\n",
        "TRAIN_EPOCHS = 100\n",
        "\n",
        "if UF == 'RS':\n",
        "    MODEL_DIR = \"/content/drive/MyDrive/Colab Notebooks/TRAIN_LANDSAT_RICE/rice_logs_MAPBIOMAS/RS\"\n",
        "    CHANNELS = 4\n",
        "elif UF == 'SCPR':\n",
        "    MODEL_DIR = \"/content/drive/MyDrive/Colab Notebooks/TRAIN_LANDSAT_RICE/rice_logs_MAPBIOMAS/SCPR\"\n",
        "    CHANNELS = 3\n",
        "elif UF == 'TO':\n",
        "    MODEL_DIR = \"/content/drive/MyDrive/Colab Notebooks/TRAIN_LANDSAT_RICE/rice_logs_MAPBIOMAS/TO\"\n",
        "    CHANNELS = 5\n",
        "\n",
        "# Predict images\n",
        "PREDICT_INPUT_DIR = \"/content/drive/MyDrive/TRAIN_LANDSAT_RICE predict\"\n",
        "PREDICT_OUPUT_DIR = \"/content/drive/MyDrive/TRAIN_LANDSAT_RICE/predicted\"\n",
        "PREDICT_CHIP_SIZE = 256\n",
        "PREDICT_GRIDS = 1\n",
        "PREDICT_BATCH_SIZE = 1\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(PREDICT_OUPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df9erxjKFAPm"
      },
      "source": [
        "# Image Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS9aROwLE1LN"
      },
      "outputs": [],
      "source": [
        "def load_file(path):\n",
        "    original_source = gdal.Open(path)\n",
        "\n",
        "    if REPROJECT:\n",
        "        new_source = reproject_dataset(original_source,\n",
        "                                    pixel_spacing=SPATIAL_SCALE,\n",
        "                                    epsg_to=PROJECTION)\n",
        "    else:\n",
        "        new_source = original_source\n",
        "\n",
        "    if not new_source is None:\n",
        "        bands = []\n",
        "        for index in range(1, new_source.RasterCount + 1):\n",
        "            band = new_source.GetRasterBand(index).ReadAsArray()\n",
        "            bands.append(band)\n",
        "\n",
        "        image = np.dstack(bands)\n",
        "\n",
        "        return original_source, new_source, image\n",
        "    else:\n",
        "        return original_source, new_source, None\n",
        "\n",
        "def normalize(image):\n",
        "    # image_mean = np.nanmean(image)\n",
        "    # image_std = np.nanstd(image)\n",
        "    # normalized = np.array( (image - image_mean) / image_std )\n",
        "    normalized = image / 255\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def get_rotate(image):\n",
        "    images = []\n",
        "    for rot in [90, 180, 270]:\n",
        "        image_rotate = rotate(image, rot, preserve_range=True)\n",
        "        images.append(image_rotate)\n",
        "    return images\n",
        "\n",
        "def get_flip(image):\n",
        "    horizontal_flip = image[:, ::-1]\n",
        "    vertical_flip = image[::-1, :]\n",
        "    return [horizontal_flip, vertical_flip]\n",
        "\n",
        "def make_dataset(filename, width, height, channels):\n",
        "    dataset = h5py.File(filename, 'w')\n",
        "\n",
        "    x_data = dataset.create_dataset(\"x\", \n",
        "                                    (0, width, height, channels), \n",
        "                                    maxshape=(None, width, height, channels),\n",
        "                                    chunks=True, \n",
        "                                    compression=COMPRESSION)\n",
        "    \n",
        "    y_data = dataset.create_dataset(\"y\", \n",
        "                                    (0, width, height, 1), \n",
        "                                    maxshape=(None, width, height, 1),\n",
        "                                    chunks=True, \n",
        "                                    compression=COMPRESSION)\n",
        "    \n",
        "    return dataset, x_data, y_data\n",
        "\n",
        "def save_dataset(X, y, output_path, chip_size, channels):\n",
        "    if os.path.isfile(output_path):\n",
        "        dataset, x_data, y_data = load_dataset(output_path)\n",
        "    else:\n",
        "        dataset, x_data, y_data = make_dataset(output_path, \n",
        "                                               chip_size,\n",
        "                                               chip_size, \n",
        "                                               channels)\n",
        "\n",
        "    length = len(X)\n",
        "\n",
        "    x_data_size = x_data.len()\n",
        "    y_data_size = y_data.len()\n",
        "\n",
        "    x_data.resize((x_data_size + length, chip_size, chip_size, channels))\n",
        "    y_data.resize((y_data_size + length, chip_size, chip_size, 1))\n",
        "\n",
        "    print(\"Saving dataset...\" , len(X))\n",
        "    \n",
        "    x_data[y_data_size:] = X\n",
        "    y_data[y_data_size:] = y\n",
        "\n",
        "    dataset.close()\n",
        "\n",
        "def load_dataset(dataset, read_only=False):\n",
        "    if read_only:\n",
        "        dataset = h5py.File(dataset, 'r')\n",
        "    else:\n",
        "        dataset = h5py.File(dataset, 'r+')\n",
        "    \n",
        "    x_data = dataset[\"x\"]\n",
        "    y_data = dataset[\"y\"]\n",
        "\n",
        "    return dataset, x_data, y_data\n",
        "\n",
        "\n",
        "def mosaic_is_empty(image):\n",
        "    empty_percentage = (np.sum(np.sum(image == 0, axis=2) == CHANNELS) / (image.shape[0] * image.shape[1])) * 100\n",
        "    return empty_percentage >= 5\n",
        "\n",
        "\n",
        "def chip_is_empty(chip):\n",
        "    unique_labels = np.unique(chip)\n",
        "    return 0 in unique_labels and len(unique_labels) == 1\n",
        "\n",
        "\n",
        "def generate_dataset(image_path, labels_path, chip_size, channels, \n",
        "                     grids=1, allow_empty_chip=False, rotate=False, flip=False):\n",
        "    _, _, image_data = load_file(image_path)\n",
        "    _, _, image_labels = load_file(labels_path)\n",
        "\n",
        "    image_labels = resize(image_labels,\n",
        "                          (image_data.shape[0], image_data.shape[1]),\n",
        "                          preserve_range=True, anti_aliasing=True).astype(np.int8)\n",
        "\n",
        "    image = np.dstack([image_data, image_labels])\n",
        "\n",
        "    X_set = []\n",
        "    y_set = []\n",
        "\n",
        "    for step in get_grids(grids, chip_size):\n",
        "        for (x, y, window, dimension) in sliding_window(image,\n",
        "                                                        step[\"steps\"],\n",
        "                                                        step[\"chip_size\"],\n",
        "                                                        (chip_size,\n",
        "                                                         chip_size)):\n",
        "\n",
        "            train = np.array(window[:, :, : channels])\n",
        "\n",
        "            labels = np.array(window[:, :, -1:], dtype=np.int8)\n",
        "\n",
        "            unique_labels = np.unique(labels)\n",
        "            \n",
        "            if (chip_is_empty(labels) or mosaic_is_empty(train)) and not allow_empty_chip:\n",
        "                continue\n",
        "\n",
        "            # if mosaic_is_empty(train) and not allow_empty_chip or set(unique_labels) != set(LABELS):\n",
        "            #     continue\n",
        "\n",
        "            raw_image = np.dstack([train, labels])\n",
        "            images_daugmentation = [raw_image]\n",
        "            \n",
        "            if rotate:\n",
        "                images_rotate = get_rotate(raw_image)\n",
        "                images_daugmentation.extend(images_rotate)\n",
        "\n",
        "            if flip:\n",
        "                images_flip = []\n",
        "                for im in images_daugmentation:\n",
        "                    images_flip.extend(get_flip(im))\n",
        "                images_daugmentation.extend(images_flip)\n",
        "\n",
        "            X_group = []\n",
        "            Y_group = []\n",
        "\n",
        "            for i in images_daugmentation:\n",
        "                new_train = np.array(i[:, :, :channels])\n",
        "                new_labels = np.array(i[:, :, -1:], dtype=np.int8)\n",
        "\n",
        "                np.clip(new_labels, 0, None, out=new_labels)\n",
        "                \n",
        "                X_group.append(new_train)\n",
        "                Y_group.append(new_labels)\n",
        "\n",
        "            X_set.append(X_group)\n",
        "            y_set.append(Y_group)\n",
        "\n",
        "        X_set = np.array(X_set)\n",
        "        y_set = np.array(y_set)\n",
        "\n",
        "        yield X_set, y_set\n",
        "\n",
        "\n",
        "def generate_train_validation_dataset(image_path, labels_path, \n",
        "                                      train_path, validation_path, test_path, \n",
        "                                      chip_size, \n",
        "                                      channels=1, \n",
        "                                      grids=1, \n",
        "                                      allow_empty_chip=False, \n",
        "                                      rotate=False, flip=False):\n",
        "    \n",
        "    for X_set, y_set in generate_dataset(image_path, labels_path, \n",
        "                                    chip_size=chip_size,\n",
        "                                    channels=channels, \n",
        "                                    grids=grids, \n",
        "                                    allow_empty_chip=allow_empty_chip, \n",
        "                                    rotate=rotate, \n",
        "                                    flip=flip):\n",
        "        \n",
        "        if len(X_set) >= 5:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X_set, y_set,\n",
        "                                                        test_size=0.30,\n",
        "                                                        random_state=1)\n",
        "            \n",
        "            X_val, X_test, y_val, y_test = train_test_split(X_val, y_val,\n",
        "                                                        test_size=0.30,\n",
        "                                                        random_state=1)\n",
        "\n",
        "            X_train =  np.array([item for sublist in X_train for item in sublist])\n",
        "            y_train =  np.array([item for sublist in y_train for item in sublist])\n",
        "\n",
        "            X_val =  np.array([item for sublist in X_val for item in sublist])\n",
        "            y_val =  np.array([item for sublist in y_val for item in sublist])\n",
        "\n",
        "            X_test =  np.array([item for sublist in X_test for item in sublist])\n",
        "            y_test =  np.array([item for sublist in y_test for item in sublist])\n",
        "\n",
        "            save_dataset(X_train, y_train, train_path, chip_size, channels)\n",
        "            save_dataset(X_val, y_val, validation_path, chip_size, channels)\n",
        "            save_dataset(X_test, y_test, test_path, chip_size, channels)\n",
        "\n",
        "def sliding_window(image, step, chip_size, chip_resize):\n",
        "    # slide a chip across the image\n",
        "    step_cols = int(step[0])\n",
        "    step_rows = int(step[1])\n",
        "\n",
        "    cols = image.shape[1]\n",
        "    rows = image.shape[0]\n",
        "\n",
        "    chip_size_cols = chip_size[0]\n",
        "    chip_size_rows = chip_size[1]\n",
        "\n",
        "    chip_resize_cols = chip_resize[0]\n",
        "    chip_resize_rows = chip_resize[1]\n",
        "\n",
        "    for y in range(0, rows, step_rows):\n",
        "        for x in range(0, cols, step_cols):\n",
        "\n",
        "            origin_x = x\n",
        "            origin_y = y\n",
        "\n",
        "            if (origin_y + chip_size_rows) > rows:\n",
        "                origin_y = rows - chip_size_rows\n",
        "\n",
        "            if (origin_x + chip_size_cols) > cols:\n",
        "                origin_x = cols - chip_size_cols\n",
        "\n",
        "            chip = image[origin_y:origin_y + chip_size_rows,\n",
        "                   origin_x: origin_x + chip_size_cols]\n",
        "\n",
        "            original_shape = chip.shape\n",
        "\n",
        "            if chip.shape != (chip_resize_cols, chip_resize_rows):\n",
        "                chip = resize(chip,\n",
        "                              (chip_resize_cols, chip_resize_rows),\n",
        "                              preserve_range=True,\n",
        "                              anti_aliasing=True)\n",
        "\n",
        "            yield (origin_x, origin_y, chip, original_shape)\n",
        "\n",
        "\n",
        "def get_window(matrix, x, y, width, height):\n",
        "    return matrix[y:y + height, x:x + width]\n",
        "\n",
        "\n",
        "def set_window(matrix, x, y, new_matrix):\n",
        "    for i_index, i in enumerate(range(y, y + new_matrix.shape[0])):\n",
        "        for j_index, j in enumerate(range(x, x + new_matrix.shape[1])):\n",
        "            matrix[i][j] = new_matrix[i_index][j_index]\n",
        "\n",
        "\n",
        "def transform_labels(labels_array, labels):\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(labels)\n",
        "    new_labels_array = []\n",
        "    for ix, l in enumerate(labels_array):\n",
        "        flat_labels = l.reshape((l.shape[0] * l.shape[1],))\n",
        "        transformed_flat_labels = lb.transform(flat_labels)\n",
        "        new_labels_array.append(transformed_flat_labels.reshape(\n",
        "            (l.shape[0], l.shape[1], len(labels))))\n",
        "\n",
        "    new_labels_array = np.array(new_labels_array)\n",
        "    return new_labels_array\n",
        "\n",
        "\n",
        "def get_grids(grids, chip_size):\n",
        "    grids_dict = {\n",
        "        1: [\n",
        "            {\"steps\": (chip_size, chip_size),\n",
        "             \"chip_size\": (chip_size, chip_size)}\n",
        "        ],\n",
        "        2: [\n",
        "            {\"steps\": (int(chip_size * 0.5), int(chip_size * 0.5)),\n",
        "             \"chip_size\": (chip_size, chip_size)},\n",
        "        ],\n",
        "        3: [\n",
        "            {\"steps\": (int(chip_size * 0.9), int(chip_size * 0.9)),\n",
        "             \"chip_size\": (chip_size, chip_size)},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return grids_dict[grids]\n",
        "\n",
        "\n",
        "def reproject_dataset(g, pixel_spacing=30., epsg_to=3857):\n",
        "    osng = osr.SpatialReference()\n",
        "    osng.ImportFromEPSG(epsg_to)\n",
        "\n",
        "    wkt = g.GetProjection()\n",
        "    wgs84 = osr.SpatialReference()\n",
        "    wgs84.ImportFromWkt(wkt)\n",
        "\n",
        "    tx = osr.CoordinateTransformation(wgs84, osng)\n",
        "    # Up to here, all  the projection have been defined, as well as a\n",
        "    # transformation from the from to the  to :)\n",
        "\n",
        "    # Get the Geotransform vector\n",
        "    geo_t = g.GetGeoTransform()\n",
        "    x_size = g.RasterXSize  # Raster xsize\n",
        "    y_size = g.RasterYSize  # Raster ysize\n",
        "    # Work out the boundaries of the new dataset in the target projection\n",
        "    (ulx, uly, ulz) = tx.TransformPoint(geo_t[0], geo_t[3])\n",
        "    (lrx, lry, lrz) = tx.TransformPoint(geo_t[0] + geo_t[1] * x_size, \\\n",
        "                                        geo_t[3] + geo_t[5] * y_size)\n",
        "\n",
        "    # Now, we create an in-memory raster\n",
        "    mem_drv = gdal.GetDriverByName('MEM')\n",
        "    # The size of the raster is given the new projection and pixel spacing\n",
        "    # Using the values we calculated above. Also, setting it to store one band\n",
        "    # and to use Float32 data type.\n",
        "    dest = mem_drv.Create('', int((lrx - ulx) / pixel_spacing), \\\n",
        "                          int((uly - lry) / pixel_spacing), g.RasterCount,\n",
        "                          g.GetRasterBand(1).DataType)\n",
        "    # Calculate the new geotransform\n",
        "    new_geo = (ulx, pixel_spacing, geo_t[2], \\\n",
        "               uly, geo_t[4], -pixel_spacing)\n",
        "    # Set the geotransform\n",
        "    dest.SetGeoTransform(new_geo)\n",
        "    dest.SetProjection(osng.ExportToWkt())\n",
        "    # Perform the projection/resampling\n",
        "    res = gdal.ReprojectImage(g, dest, \\\n",
        "                              wgs84.ExportToWkt(), osng.ExportToWkt(), \\\n",
        "                              gdal.GRA_NearestNeighbour)\n",
        "    return dest\n",
        "\n",
        "\n",
        "\n",
        "def save_results(original_dataset, reprojected_dataset, image, output_path):\n",
        "    mem_dataset = reprojected_dataset \\\n",
        "        .GetDriver() \\\n",
        "        .Create(output_path, image.shape[1], image.shape[0], 1, gdal.GDT_Int16)\n",
        "\n",
        "    mem_dataset.SetGeoTransform(reprojected_dataset.GetGeoTransform())\n",
        "    mem_dataset.SetProjection(reprojected_dataset.GetProjection())\n",
        "    mem_dataset.GetRasterBand(1) \\\n",
        "        .WriteArray(image.reshape((image.shape[0], image.shape[1])), 0, 0)\n",
        "    mem_dataset.FlushCache()\n",
        "\n",
        "    original_epsg = int(osr \\\n",
        "                        .SpatialReference(wkt=original_dataset.GetProjection()) \\\n",
        "                        .GetAttrValue('AUTHORITY', 1))\n",
        "\n",
        "    output_dataset = reproject_dataset(mem_dataset,\n",
        "                                       SPATIAL_SCALE,\n",
        "                                       original_epsg)\n",
        "\n",
        "    original_dataset.GetDriver().CreateCopy(output_path,\n",
        "                                            output_dataset,\n",
        "                                            options=['COMPRESS=LZW',\n",
        "                                                     'TFW=YES'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOCNbbIp03Df"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcPGwC-u9G0Z"
      },
      "source": [
        "## Define metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVPzR_5u8_x0"
      },
      "outputs": [],
      "source": [
        "def jaccard_coef(y_true, y_pred):\n",
        "    smooth = 1e-12\n",
        "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
        "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return K.mean(jac)\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b458qR8pRMU"
      },
      "source": [
        "## U-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9uL77bj08t6"
      },
      "outputs": [],
      "source": [
        "def conv2d_block(n_filters, kernel_size, activation='relu', inputs=None):\n",
        "    net = Conv2D(filters=n_filters, \n",
        "                 kernel_size=kernel_size, \n",
        "                 activation=None,\n",
        "                 kernel_initializer='he_normal',\n",
        "                 padding='same') (inputs) \n",
        "    net = BatchNormalization()(net)\n",
        "    net = Activation(activation)(net)\n",
        "\n",
        "    net = Conv2D(filters=n_filters, \n",
        "                 kernel_size=kernel_size, \n",
        "                 activation=None,\n",
        "                 kernel_initializer='he_normal', \n",
        "                 padding='same') (net) \n",
        "    net = BatchNormalization()(net)\n",
        "    net = Activation(activation)(net)\n",
        "\n",
        "    return net\n",
        "\n",
        "def transpose(n_filters, kernel_size, inputs=None):\n",
        "    net = Conv2DTranspose(n_filters, \n",
        "                          kernel_size, \n",
        "                          strides=(2, 2),\n",
        "                          padding='same') (inputs)\n",
        "    return net\n",
        "\n",
        "def model_fn(input_shape, n_filters=16, dropout = 0.5, labels=[]): # parâmetro n_filters\n",
        "    inputs = keras.Input(input_shape)\n",
        "\n",
        "    c1 = conv2d_block(n_filters * 1, (3, 3), inputs=inputs)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = conv2d_block(n_filters * 2, (3, 3), inputs=p1)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = conv2d_block(n_filters * 4, (3, 3), inputs=p2)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = conv2d_block(n_filters * 8, (3, 3), inputs=p3)\n",
        "    c4 = Dropout(dropout) (c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "    c5 = conv2d_block(n_filters * 16, (3, 3), inputs=p4)\n",
        "    c5 = Dropout(dropout) (c5)\n",
        "\n",
        "    u6 = transpose(n_filters * 8, (2, 2), inputs=c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = conv2d_block(n_filters * 8, (2, 2), inputs=u6)\n",
        "    \n",
        "    u7 = transpose(n_filters * 4, (2, 2), inputs=c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = conv2d_block(n_filters * 4, (2, 2), inputs=u7)\n",
        "\n",
        "    u8 = transpose(n_filters * 2, (2, 2), inputs=c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = conv2d_block(n_filters * 2, (2, 2), inputs=u8)\n",
        "    \n",
        "    u9 = transpose(n_filters * 1, (2, 2), inputs=c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = conv2d_block(n_filters * 1, (2, 2), inputs=u9)\n",
        "    \n",
        "    if len(labels) > 2:\n",
        "        outputs = tf.keras.layers.Conv2D(len(labels), 1, 1, \n",
        "                                         activation='sigmoid')(c9)\n",
        "        loss_function = categorical_focal_loss()\n",
        "        metrics = [jaccard_coef, dice_coef]\n",
        "    else:\n",
        "        outputs = tf.keras.layers.Conv2D(1, (1, 1), \n",
        "                                         activation='sigmoid')(c9)\n",
        "        loss_function = 'binary_crossentropy'\n",
        "        metrics = [jaccard_coef, dice_coef]\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss=loss_function, \n",
        "                  metrics=[jaccard_coef, dice_coef])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyfS7rI82Dj4"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngmo5JVC2DzF"
      },
      "outputs": [],
      "source": [
        "class Classifier(object):\n",
        "    def __init__(self, chip_size, channels, model_dir, labels, to_save_model=False):\n",
        "        self.__chip_size = chip_size\n",
        "        self.__channels = channels\n",
        "        self.__labels = labels\n",
        "\n",
        "        if to_save_model:\n",
        "            self.__model = model_fn((None, None, channels), labels=labels)\n",
        "        else:\n",
        "            self.__model = model_fn((chip_size, chip_size, channels), labels=labels)\n",
        "\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "\n",
        "        self.load_model(model_dir)\n",
        "        self.load_callbacks(model_dir)\n",
        "\n",
        "    def load_model(self, model_dir):\n",
        "        latest = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "        if latest:\n",
        "            print('Loading model....')\n",
        "            self.__model.load_weights(latest)\n",
        "\n",
        "        print('Model loaded!')\n",
        "        self.__model.summary()\n",
        "        tf.keras.utils.plot_model(self.__model, show_shapes=True)\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.__model\n",
        "\n",
        "    def load_callbacks(self, model_dir):\n",
        "        es_cp_callback = tf.keras.callbacks. \\\n",
        "            EarlyStopping(patience=20, \n",
        "                          verbose=1,\n",
        "                          restore_best_weights=True)\n",
        "        \n",
        "        checkpoint_path = '{dir}/model.ckpt'.format(dir=model_dir)\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_path,\n",
        "            save_weights_only=True,\n",
        "            save_best_only=True)\n",
        "\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=model_dir)\n",
        "\n",
        "        self.__callbacks = [es_cp_callback, cp_callback, tensorboard_callback]\n",
        "\n",
        "\n",
        "    def train(self, input_train, input_validation, epochs, batch_size, generator=None, data_porportion=None):\n",
        "        train_dataset, train_data, train_labels = load_dataset(input_train,\n",
        "                                                            read_only=True)\n",
        "        validation_dataset, validation_data, validation_labels = load_dataset(\n",
        "            input_validation, read_only=True)\n",
        "        \n",
        "        print(train_data.shape)\n",
        "        print(validation_data.shape)\n",
        "\n",
        "        if data_porportion not in [None, 1]:\n",
        "            train_proportion = int(len(train_data) * data_porportion)\n",
        "            validation_proportion = int(len(validation_data) * data_porportion)\n",
        "\n",
        "            train_data = train_data[:train_proportion]\n",
        "            train_labels = train_labels[:train_proportion]\n",
        "            validation_data = validation_data[:train_proportion]\n",
        "            validation_labels = validation_labels[:train_proportion]\n",
        "            \n",
        "        if generator:\n",
        "            history = self.__model.fit(generator(train_data, train_labels, batch_size, self.__labels), \n",
        "                    steps_per_epoch=len(train_data)//batch_size, \n",
        "                    epochs=epochs,\n",
        "                    validation_data=generator(validation_data, validation_labels, batch_size, self.__labels),\n",
        "                    validation_steps=len(validation_data)//batch_size,\n",
        "                    callbacks=self.__callbacks)\n",
        "            \n",
        "        else:\n",
        "            train_images = np.asarray(train_data, dtype=np.float32)\n",
        "            train_labels = np.asarray(train_labels, dtype=np.int8)\n",
        "\n",
        "            validation_images = np.asarray(validation_data, dtype=np.float32)\n",
        "            validation_labels = np.asarray(validation_labels, dtype=np.int8)\n",
        "\n",
        "            if len(self.__labels) > 2:\n",
        "                train_labels = transform_labels(train_labels, self.__labels)\n",
        "                validation_labels = transform_labels(validation_labels,\n",
        "                                                    self.__labels)\n",
        "\n",
        "            history = self.__model.fit(x=train_images,\n",
        "                            y=train_labels,\n",
        "                            validation_data=(validation_images, validation_labels),\n",
        "                            epochs=epochs,\n",
        "                            batch_size=batch_size,\n",
        "                            verbose=1,\n",
        "                            callbacks=self.__callbacks)\n",
        "        \n",
        "        fig, ax = plt.subplots(3,1, figsize=(15, 10))\n",
        "        ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
        "        ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
        "        legend = ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "        ax[1].plot(history.history['jaccard_coef'], color='b', label=\"Training Jaccard Index\")\n",
        "        ax[1].plot(history.history['val_jaccard_coef'], color='r',label=\"Validation Jaccard Index\")\n",
        "        legend = ax[1].legend(loc='best', shadow=True)\n",
        "\n",
        "        ax[2].plot(history.history['dice_coef'], color='b', label=\"Training Dice Index\")\n",
        "        ax[2].plot(history.history['val_dice_coef'], color='r',label=\"Validation Dice Index\")\n",
        "        legend = ax[2].legend(loc='best', shadow=True)\n",
        "\n",
        "        plt.plot()\n",
        "\n",
        "        train_dataset.close()\n",
        "        validation_dataset.close()\n",
        "\n",
        "    def evaluate(self, input_test):\n",
        "        test_dataset, test_data, test_labels = load_dataset(input_test,\n",
        "                                                            read_only=True)\n",
        "        \n",
        "        test_images = normalize(np.asarray(test_data, dtype=np.float32))\n",
        "        test_labels = np.asarray(test_labels, dtype=np.int8)\n",
        "\n",
        "        if len(self.__labels) > 2:\n",
        "            test_labels = transform_labels(test_labels, self.__labels)\n",
        "\n",
        "        self.__model.evaluate(x=test_images,\n",
        "                            y=test_labels,\n",
        "                            batch_size=1)\n",
        "        \n",
        "\n",
        "    def predict(self, input_path, output_path, grids, batch_size):\n",
        "        original_dataset, input_dataset, image = load_file(input_path)\n",
        "\n",
        "        print('dataset loaded')\n",
        "\n",
        "        image = image[:, :, : self.__channels]\n",
        "\n",
        "        image = normalize(image)\n",
        "\n",
        "        predicted_image = np.zeros((image.shape[0], image.shape[1]),\n",
        "                                   dtype=np.int8)\n",
        "\n",
        "        grids = get_grids(grids, self.__chip_size)\n",
        "\n",
        "        for step in grids:\n",
        "            batch = []\n",
        "            windows = sliding_window(image, step['steps'], step['chip_size'],\n",
        "                                     (self.__chip_size, self.__chip_size))\n",
        "\n",
        "            for (x, y, chip, original_dimensions) in tqdm(iterable=windows,\n",
        "                                                          miniters=10,\n",
        "                                                          unit=' windows'):\n",
        "\n",
        "                batch.append({'chip': chip, 'x': x, 'y': y, \n",
        "                              'dimensions': original_dimensions})\n",
        "\n",
        "                if len(batch) >= batch_size:\n",
        "                    chips = []\n",
        "                    positions = []\n",
        "                    dimensions = []\n",
        "\n",
        "                    for b in batch:\n",
        "                        chips.append(b.get('chip'))\n",
        "                        positions.append((b.get('x'), b.get('y')))\n",
        "                        dimensions.append(b.get('dimensions'))\n",
        "\n",
        "                    chips = np.array(chips, dtype=np.float32)\n",
        "\n",
        "                    pred = self.__model.predict(chips, batch_size=batch_size)\n",
        "\n",
        "                    for chip, position, dimension, predict in zip(chips,\n",
        "                                                                  positions,\n",
        "                                                                  dimensions,\n",
        "                                                                  pred):\n",
        "\n",
        "                        if len(self.__labels) > 2:\n",
        "                            predict = np.array(tf.math.argmax(predict, axis=2))\n",
        "                        else:\n",
        "                            predict[predict > 0.5] = 1\n",
        "                            predict[predict <= 0.5] = 0\n",
        "\n",
        "                        predict = resize(predict, (dimension[0], dimension[1]),\n",
        "                                         preserve_range=True,\n",
        "                                         anti_aliasing=True).astype(np.int8)\n",
        "\n",
        "                        predict = predict.reshape(\n",
        "                            (predict.shape[0], predict.shape[1]))\n",
        "\n",
        "                        predicted = get_window(predicted_image,\n",
        "                                               position[0],\n",
        "                                               position[1],\n",
        "                                               predict.shape[1],\n",
        "                                               predict.shape[0])\n",
        "\n",
        "                        if predict.shape != predicted.shape:\n",
        "                            raise Exception('predict.shape != predicted.shape')\n",
        "\n",
        "                        if len(self.__labels) > 2:\n",
        "                            set_window(predicted_image, position[0],\n",
        "                                       position[1], predict)\n",
        "                        else:\n",
        "                            set_window(predicted_image, position[0],\n",
        "                                       position[1], np.add(predict, predicted))\n",
        "\n",
        "                    batch = []\n",
        "\n",
        "            if len(self.__labels) == 2:\n",
        "                predicted_image[predicted_image >= 1] = 1\n",
        "                \n",
        "            print('Saving results...')\n",
        "            save_results(original_dataset, input_dataset, predicted_image,\n",
        "                      output_path)\n",
        "\n",
        "            del original_dataset, input_dataset, image, predicted_image\n",
        "            gc.collect()\n",
        "            print('Finished!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQRqOn292cJs"
      },
      "source": [
        "# Predict Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDcbK0cn2cfc",
        "outputId": "51be654d-e92d-44ca-c5f4-d5a3b21ac871"
      },
      "outputs": [],
      "source": [
        "classifier = Classifier(chip_size=PREDICT_CHIP_SIZE,\n",
        "                        channels=CHANNELS,\n",
        "                        model_dir=MODEL_DIR,\n",
        "                        labels=LABELS)\n",
        "\n",
        "files = [f for f in listdir(PREDICT_INPUT_DIR) if\n",
        "         isfile(join(PREDICT_INPUT_DIR, f))]\n",
        "\n",
        "if len(files) == 0:\n",
        "    print(\"No file found.\")\n",
        "\n",
        "for f in files:\n",
        "    print(\"File:\", f)\n",
        "    input_file = \"{directory}/{filepath}\".format(directory=PREDICT_INPUT_DIR, filepath=f)\n",
        "\n",
        "    output_file = \"{directory}/{filepath}\".format(directory=PREDICT_OUPUT_DIR, filepath=f)\n",
        "\n",
        "    if not os.path.exists(PREDICT_OUPUT_DIR):\n",
        "            os.makedirs(PREDICT_OUPUT_DIR)\n",
        "\n",
        "    if exists(output_file):\n",
        "        print(\"File {} exists. Skipping...\".format(output_file))\n",
        "        continue\n",
        "\n",
        "    print(\"Predict: \", input_file, \"  >>  \", output_file)\n",
        "\n",
        "    classifier.predict(input_path=input_file, output_path=output_file,\n",
        "                       grids=PREDICT_GRIDS,\n",
        "                       batch_size=PREDICT_BATCH_SIZE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5T7SsB0sFHZo",
        "TJ2koetlt85z",
        "Df9erxjKFAPm",
        "NcPGwC-u9G0Z",
        "_b458qR8pRMU",
        "oyfS7rI82Dj4"
      ],
      "machine_shape": "hm",
      "name": "semantic_segmentation_rice_classify.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
